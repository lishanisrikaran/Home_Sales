{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_KW73O2e3dw",
        "outputId": "a7697a8b-b00d-4607-8cae-a27ffffbd2d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com] [1 InRelease 14.2 kB/110 kB 13%] [Connect\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [1 InRelease 67.7 kB/110 kB 61%] [2 InRelease 0 B/3,62\r0% [Waiting for headers] [1 InRelease 90.9 kB/110 kB 82%] [Connected to ppa.lau\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,283 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,260 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [21.8 kB]\n",
            "Fetched 3,003 kB in 2s (1,340 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "# Sets up the Apache Spark environment by importing the necessary libraries and installations:\n",
        "\n",
        "import os                                                                                                         # Enables interaction with the operating system\n",
        "\n",
        "\n",
        "# Defines the latest version of Apache Spark version (currently 3.4.0) and sets it up in the operating system's environment variables\n",
        "# The latest version  of Apache Spark can be found on http://www.apache.org/dist/spark/\n",
        "spark_version = 'spark-3.4.0'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Installs Apache Spark and Java (Apache Spark requires Java to run)\n",
        "# The exclamation mark '!' indicates that the line is ran in the terminal environment\n",
        "!apt-get update                                                                                                   # Ensures the package manager has the latest information about the available software packages\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null                                                          # Installs the OpenJDK 11 which is a Java Development kit\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz                           # Visits the Apache website and gets the tgz file that contains the compressed Spark packaged for Hadoop\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz                                                                            # Unpacks Spark into the current directory\n",
        "!pip install -q findspark                                                                                         # Installs findspark which enables the Python environment to work with the installed Spark version (3.4.0)\n",
        "\n",
        "# Sets up environment Variables for Java and Apache Spark installations (pointing to where the installation is located)\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Starts a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2XbWNf1Te5fM"
      },
      "outputs": [],
      "source": [
        "# Imports libraries and dependencies required\n",
        "from pyspark.sql import SparkSession                                                                              # Enables the creation and management of Spark sessions where the session acts as a end point to work with the Apache Spark API\n",
        "import time                                                                                                       # Enables time related operations in Python\n",
        "from pyspark import SparkFiles                                                                                    # Enables accessability to distributed files\n",
        "\n",
        "# Sets up a SparkSession in a PySpark application\n",
        "# Configurations include showing up to 2000 fields and increasing the default driver memory from 1g to 2g to avoid crashing due to the dataset's size\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkSQL\") \\\n",
        "    .config(\"spark.sql.debug.maxToStringFields\", 2000) \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wOJqxG_RPSwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035290c8-a730-4850-f3a6-5c8f0f66b79b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|id                                  |date_sold |year_built|price |bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
            "+------------------------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|f8a53099-ba1c-47d6-9c31-7398aa8f6089|2022-04-08|2016      |936923|4       |3        |3167       |11733   |2     |1         |76  |\n",
            "|7530a2d8-1ae3-4517-9f4a-befe060c4353|2021-06-13|2013      |379628|2       |2        |2235       |14384   |1     |0         |23  |\n",
            "|43de979c-0bf0-4c9f-85ef-96dc27b258d5|2019-04-12|2014      |417866|2       |2        |2127       |10575   |2     |0         |0   |\n",
            "|b672c137-b88c-48bf-9f18-d0a4ac62fb8b|2019-10-16|2016      |239895|2       |2        |1631       |11149   |2     |0         |0   |\n",
            "|e0726d4d-d595-4074-8283-4139a54d0d63|2022-01-08|2017      |424418|3       |2        |2249       |13878   |2     |0         |4   |\n",
            "|5aa00529-0533-46ba-870c-9e881580ef35|2019-01-30|2017      |218712|2       |3        |1965       |14375   |2     |0         |7   |\n",
            "|131492a1-72e2-4a84-bf97-0db14973bfdb|2020-02-08|2017      |419199|2       |3        |2062       |8876    |2     |0         |6   |\n",
            "|8d54a71b-c520-44e5-8ba1-5a84be03ad35|2019-07-21|2010      |323956|2       |3        |1506       |11816   |1     |0         |25  |\n",
            "|e81aacfe-17fe-46b1-a52a-4753d1622b4a|2020-06-16|2016      |181925|3       |3        |2137       |11709   |2     |0         |22  |\n",
            "|2ed8d509-7372-46d5-a9dd-9281a95467d4|2021-08-06|2015      |258710|3       |3        |1918       |9666    |1     |0         |25  |\n",
            "|f876d86f-3c9f-42b6-928f-bcb685c070ed|2019-02-27|2011      |167864|3       |3        |2471       |13924   |2     |0         |15  |\n",
            "|0a2bd445-8508-4d83-b6be-f043bf3f3b24|2021-12-30|2014      |337527|2       |3        |1926       |12556   |1     |0         |23  |\n",
            "|941bad30-eb49-4a78-b83a-87abb87a62db|2020-05-09|2015      |229896|3       |3        |2197       |8641    |1     |0         |3   |\n",
            "|dd61eb34-6589-4c04-92ba-fc3c44f1b670|2021-07-25|2016      |210247|3       |2        |1672       |11986   |2     |0         |28  |\n",
            "|f1e4cef7-d151-4391-8d00-d92ed2b7ab1b|2019-02-01|2011      |398667|2       |3        |2331       |11356   |1     |0         |7   |\n",
            "|ea620c7b-c2f7-4c6f-9d16-3ccb34320ca4|2021-05-31|2011      |437958|3       |3        |2356       |11052   |1     |0         |26  |\n",
            "|f233cb41-6f33-4b0c-b9ed-1b8aa87163e5|2021-07-18|2016      |437375|4       |3        |1704       |11721   |2     |0         |34  |\n",
            "|c797ca12-52cd-4b13-9338-183653619b11|2019-06-08|2015      |288650|2       |3        |2100       |10419   |2     |0         |7   |\n",
            "|0cfe57f3-28c2-472c-9bc3-aaeac6807e62|2019-10-04|2015      |308313|3       |3        |1960       |9453    |2     |0         |2   |\n",
            "|4566cd2a-ac6e-4358-b1ad-56921991ff60|2019-07-15|2016      |177541|3       |3        |2130       |10517   |2     |0         |25  |\n",
            "+------------------------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "only showing top 20 rows\n",
            "\n",
            "Data types within home_sales_df:\n",
            " [('id', 'string'), ('date_sold', 'date'), ('year_built', 'string'), ('price', 'int'), ('bedrooms', 'int'), ('bathrooms', 'int'), ('sqft_living', 'int'), ('sqft_lot', 'int'), ('floors', 'int'), ('waterfront', 'int'), ('view', 'int')]\n"
          ]
        }
      ],
      "source": [
        "# 1. Reads in the AWS S3 bucket into a home sales DataFrame.\n",
        "\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\" # Location of home sales revised CSV file which is within an AWS S3 bucket in the cloud\n",
        "\n",
        "spark.sparkContext.addFile(url)                                                                                    # Adds the file locally to all the nodes in the Spark cluster used to run the script\n",
        "\n",
        "home_sales_df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), sep= \",\", header=True,                    # Reads the home sales revised CSV and loads it into a DataFrame; seperates values by a comma delimiter and recognises that the data has a header record included\n",
        "                                                                                   ignoreLeadingWhiteSpace=True,   # Removes white space before values\n",
        "                                                                                   inferSchema=True,               # Observes data and interprets their data types\n",
        "                                                                                   timestampFormat=\"yyyy/MM/dd\")   # Expected format of dates in the file\n",
        "\n",
        "# The date_built column is inferred to be an integer data type which is not correct in this context, if this field is converted to a date format 'yyyy' the appearance of this column changes to a format of \"20XX-01-01\"\n",
        "# Therefore to avoid misinterpreation of the data in this field, it is changed back to a string data type\n",
        "home_sales_df = home_sales_df.withColumn(\"date_built\", home_sales_df[\"date_built\"].cast(\"string\"))\n",
        "\n",
        "\n",
        "# Renames the date and date_built columns to more applicable names\n",
        "home_sales_df = home_sales_df.withColumnRenamed(\"date\", \"date_sold\")\n",
        "home_sales_df = home_sales_df.withColumnRenamed(\"date_built\", \"year_built\")\n",
        "\n",
        "\n",
        "# Shows the home sales DataFrame ensuring that all data is visible within each field\n",
        "home_sales_df.show(truncate=False)\n",
        "\n",
        "# Prints out a list of the data types in the home sales DataFrame\n",
        "print(\"Data types within home_sales_df:\\n\", home_sales_df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RoljcJ7WPpnm"
      },
      "outputs": [],
      "source": [
        "# 2. Creates a local temporary view of the DataFrame.\n",
        "# This lazily loads the spark table into memory and enables us to query the data in Apache Spark using SQL languages\n",
        "\n",
        "home_sales_df.createOrReplaceTempView('home_sales')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this script, Apache SparkSQL queries will be used to achieve the desired outputs. It should also be acknowledged that it is also possible to use the Apache PySpark DataFrame operations such as filter, groupBy, and agg to achieve the same results however, SparkSQL is preferred as it will utilise faster performance due to being queried against a temporary table which is lazily loaded."
      ],
      "metadata": {
        "id": "GAevR1xKl4QS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L6fkwOeOmqvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26045617-c66a-4e1e-b2f0-ca94b2b714f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------+\n",
            "|Average Price|Year Sold|\n",
            "+-------------+---------+\n",
            "|  $300,263.70|     2019|\n",
            "|  $298,353.78|     2020|\n",
            "|  $301,819.44|     2021|\n",
            "|  $296,363.88|     2022|\n",
            "+-------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. Returns the average price for a four bedroom house sold in each year rounded to two decimal places and formatted as a currency\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  CONCAT('$', FORMAT_NUMBER(AVG(price), 2)) AS `Average Price`,\n",
        "  YEAR(date_sold) AS `Year Sold`\n",
        "FROM\n",
        "  home_sales\n",
        "WHERE\n",
        "  bedrooms = 4\n",
        "GROUP BY\n",
        "  `Year Sold`\n",
        "ORDER BY\n",
        "  `Year Sold`\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l8p_tUS8h8it",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e70e5e7-54f5-48bb-cd0f-6547b7bda0a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+\n",
            "|Average Price|Year Built|\n",
            "+-------------+----------+\n",
            "|  $292,859.62|      2010|\n",
            "|  $291,117.47|      2011|\n",
            "|  $293,683.19|      2012|\n",
            "|  $295,962.27|      2013|\n",
            "|  $290,852.27|      2014|\n",
            "|  $288,770.30|      2015|\n",
            "|  $290,555.07|      2016|\n",
            "|  $292,676.79|      2017|\n",
            "+-------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4. Returns the average price of a home for each year the home was built that has 3 bedrooms and 3 bathrooms rounded to two decimal places and formatted as a currency\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  CONCAT('$', FORMAT_NUMBER(AVG(price), 2)) AS `Average Price`,\n",
        "  year_built AS `Year Built`\n",
        "FROM\n",
        "  home_sales\n",
        "WHERE\n",
        "  bedrooms = 3 AND bathrooms = 3\n",
        "GROUP BY\n",
        "  `Year Built`\n",
        "ORDER BY\n",
        "  `Year Built`\n",
        "\"\"\"\n",
        "spark.sql(query).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y-Eytz64liDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c986d0-2b37-49f7-f253-8d11823e0ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+\n",
            "|Average Price|Year Built|\n",
            "+-------------+----------+\n",
            "|  $285,010.22|      2010|\n",
            "|  $276,553.81|      2011|\n",
            "|  $307,539.97|      2012|\n",
            "|  $303,676.79|      2013|\n",
            "|  $298,264.72|      2014|\n",
            "|  $297,609.97|      2015|\n",
            "|  $293,965.10|      2016|\n",
            "|  $280,317.58|      2017|\n",
            "+-------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. Returns the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors, and are greater than or equal to 2,000 square feet rounded to two decimal places and formatted as a currency\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  CONCAT('$', FORMAT_NUMBER(AVG(price), 2)) AS `Average Price`,\n",
        "  year_built AS `Year Built`\n",
        "FROM\n",
        "  home_sales\n",
        "WHERE\n",
        "  bedrooms = 3 AND\n",
        "  bathrooms = 3 AND\n",
        "  floors = 2 AND\n",
        "  sqft_living >= 2000\n",
        "GROUP BY\n",
        "  `Year Built`\n",
        "ORDER BY\n",
        "  `Year Built`\n",
        "\"\"\"\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUrfgOX1pCRd",
        "outputId": "5255e796-4b18-42c1-f306-06fe4a7c4a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----+\n",
            "|Average Price|View|\n",
            "+-------------+----+\n",
            "|  $403,848.51|   0|\n",
            "|  $401,044.25|   1|\n",
            "|  $397,389.25|   2|\n",
            "|  $398,867.60|   3|\n",
            "|  $399,631.89|   4|\n",
            "|  $401,471.82|   5|\n",
            "|  $395,655.38|   6|\n",
            "|  $403,005.77|   7|\n",
            "|  $398,592.71|   8|\n",
            "|  $401,393.34|   9|\n",
            "|  $401,868.43|  10|\n",
            "|  $399,548.12|  11|\n",
            "|  $401,501.32|  12|\n",
            "|  $398,917.98|  13|\n",
            "|  $398,570.03|  14|\n",
            "|  $404,673.30|  15|\n",
            "|  $399,586.53|  16|\n",
            "|  $398,474.49|  17|\n",
            "|  $399,332.91|  18|\n",
            "|  $398,953.17|  19|\n",
            "+-------------+----+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.9378397464752197 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 6. Returns the \"view\" rating for the average price of a home which is rounded to two decimal places and formatted as a currency, where the homes are greater than or equal to $350,000\n",
        "# The runtime for the query is also output\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  CONCAT('$', FORMAT_NUMBER(AVG(price), 2)) AS `Average Price`,\n",
        "  View AS `View`\n",
        "FROM\n",
        "  home_sales\n",
        "WHERE\n",
        "  price >= 350000\n",
        "GROUP BY\n",
        "  `View`\n",
        "ORDER BY\n",
        "  `View`\n",
        "\"\"\"\n",
        "spark.sql(query).show()\n",
        "\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KAhk3ZD2tFy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc420b0-970b-4077-8666-3d08abe24246"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# 7. Caches the the temporary table home_sales\n",
        "spark.sql(\"cache table home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4opVhbvxtL-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdfd6265-8d7d-4619-e21c-a7319b7ea44a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# 8. Checks if the temporary table home_sales is cached\n",
        "spark.catalog.isCached('home_sales')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GnL46lwTSEk",
        "outputId": "7e01fd28-b5f5-45d7-966d-93568e4811d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----+\n",
            "|Average Price|View|\n",
            "+-------------+----+\n",
            "|  $403,848.51|   0|\n",
            "|  $401,044.25|   1|\n",
            "|  $397,389.25|   2|\n",
            "|  $398,867.60|   3|\n",
            "|  $399,631.89|   4|\n",
            "|  $401,471.82|   5|\n",
            "|  $395,655.38|   6|\n",
            "|  $403,005.77|   7|\n",
            "|  $398,592.71|   8|\n",
            "|  $401,393.34|   9|\n",
            "|  $401,868.43|  10|\n",
            "|  $399,548.12|  11|\n",
            "|  $401,501.32|  12|\n",
            "|  $398,917.98|  13|\n",
            "|  $398,570.03|  14|\n",
            "|  $404,673.30|  15|\n",
            "|  $399,586.53|  16|\n",
            "|  $398,474.49|  17|\n",
            "|  $399,332.91|  18|\n",
            "|  $398,953.17|  19|\n",
            "+-------------+----+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.5915303230285645 seconds ---\n",
            "\n",
            "When the temporary table home sales is cached, the run time of the same query executes faster compared to when uncached in step 6. This is because the data is small enough to fit into memory and will be called frequently.\n"
          ]
        }
      ],
      "source": [
        "# 9. Using the cached data, returns the \"view\" rating for the average price of a home which is rounded to two decimal places and formatted as a currency, where the homes are greater than or equal to $350,000\n",
        "# The runtime for the query is also output\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  CONCAT('$', FORMAT_NUMBER(AVG(price), 2)) AS `Average Price`,\n",
        "  view AS `View`\n",
        "FROM\n",
        "  home_sales\n",
        "WHERE\n",
        "  price >= 350000\n",
        "GROUP BY\n",
        "  `View`\n",
        "ORDER BY\n",
        "  `View`\n",
        "\"\"\"\n",
        "spark.sql(query).show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(\"\\nWhen the temporary table home sales is cached, the run time of the same query executes faster compared to when uncached in step 6. This is because the data is small enough to fit into memory and will be called frequently.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Qm12WN9isHBR"
      },
      "outputs": [],
      "source": [
        "# 10. Partitions by the \"year_built\" field on the formatted parquet home sales data\n",
        "home_sales_df.write.partitionBy('year_built').mode(\"overwrite\").parquet(\"home_sales_partitioned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AZ7BgY61sRqY"
      },
      "outputs": [],
      "source": [
        "# 11. Reads the parquet formatted data\n",
        "p_home_sales_df_p = spark.read.parquet('home_sales_partitioned')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "J6MJkHfvVcvh"
      },
      "outputs": [],
      "source": [
        "# 12. Creates a local temporary table for the partitioned parquet data\n",
        "# This lazily loads the spark table into memory and enables us to query the data in Apache Spark using SQL languages\n",
        "p_home_sales_df_p.createOrReplaceTempView('p_home_sales_p')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_Vhb52rU1Sn",
        "outputId": "8baaa7f2-d3f8-4c0b-a441-75cebbd9251f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----+\n",
            "|Average Price|View|\n",
            "+-------------+----+\n",
            "|  $403,848.51|   0|\n",
            "|  $401,044.25|   1|\n",
            "|  $397,389.25|   2|\n",
            "|  $398,867.60|   3|\n",
            "|  $399,631.89|   4|\n",
            "|  $401,471.82|   5|\n",
            "|  $395,655.38|   6|\n",
            "|  $403,005.77|   7|\n",
            "|  $398,592.71|   8|\n",
            "|  $401,393.34|   9|\n",
            "|  $401,868.43|  10|\n",
            "|  $399,548.12|  11|\n",
            "|  $401,501.32|  12|\n",
            "|  $398,917.98|  13|\n",
            "|  $398,570.03|  14|\n",
            "|  $404,673.30|  15|\n",
            "|  $399,586.53|  16|\n",
            "|  $398,474.49|  17|\n",
            "|  $399,332.91|  18|\n",
            "|  $398,953.17|  19|\n",
            "+-------------+----+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 1.2631134986877441 seconds ---\n",
            "\n",
            "When the home sales data is formatted to parquet and partioned by the year built, the run time of the same query executes slower compared to when ran on the cached, unpartioned, and unformatted data in step 9. \n",
            "This is because parquet works best with large datasets such as terabytes/ petabytes and the partioning column, date built could have caused more data shuffling due to this field not being aggregated by in the query or it could have beeen distributed unequally.\n",
            "As this data set is smaller, caching seems to optimise queries the best.\n"
          ]
        }
      ],
      "source": [
        "# 13. Using the parquet partioned p_home_sales_p temporary view, a query is ran which  returns the \"view\" rating for the average price of a home which is rounded to two decimal places and formatted as a currency, where the homes are greater than or equal to $350,000\n",
        "# The runtime for the query is also output\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  CONCAT('$', FORMAT_NUMBER(AVG(price), 2)) AS `Average Price`,\n",
        "  view AS `View`\n",
        "FROM\n",
        "  p_home_sales_p\n",
        "WHERE\n",
        "  price >= 350000\n",
        "GROUP BY\n",
        "  `View`\n",
        "ORDER BY\n",
        "  `View`\n",
        "\"\"\"\n",
        "spark.sql(query).show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(\"\\nWhen the home sales data is formatted to parquet and partioned by the year built, the run time of the same query executes slower compared to when ran on the cached, unpartioned, and unformatted data in step 9. \\nThis is because parquet works best with large datasets such as terabytes/ petabytes and the partioning column, date built could have caused more data shuffling due to this field not being aggregated by in the query or it could have beeen distributed unequally.\\nAs this data set is smaller, caching seems to optimise queries the best.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hjjYzQGjtbq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c2ecc8-c31f-4093-9238-2cd2099f46a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# 14. Uncache the home_sales temporary table.\n",
        "spark.sql(\"uncache table home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Sy9NBvO7tlmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e4d710-f895-4e01-b83f-478030fac87f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# 15. Check if the home_sales temporary table is no longer cached\n",
        "spark.catalog.isCached(\"home_sales\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}